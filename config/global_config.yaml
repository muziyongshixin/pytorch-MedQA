data:
  dataset:
    train_path: /m/liyz/MedQA-workspace/rawdata/es_result_0713_production_idx/v6/v6_train.json.310000
    dev_path: /m/liyz/MedQA-workspace/rawdata/es_result_0713_production_idx/v6/v6_dev.json.13441
    test_path: /m/liyz/MedQA-workspace/rawdata/es_result_0713_production_idx/v6/v6_test.json.600
  dataset_h5: /m/liyz/MedQA-workspace/v1.0/data/v6_31w_answerfirst_with366kembed_qas100_cont100_q60_a12_withlogicAndCategory.h5  #保存预处理的数据，embedding，w2id等
  ignore_max_len: 100 # in train data, context token len > ignore_max_len will be dropped
  ignore_max_ques_ans_len: 100
  ignore_max_question_len: 60
  ignore_max_ans_len: 12

  embedding_path: data/corpus_0711_366k_200.txt
  vocabulary_path: data/vocabulary
  vacabulary_nums: 365552

  model_path:  checkpoint/panda3_2018-07-30-11_28_15_continue_panda2_2018-07-28-17_07_30_testdropoutgate0.2,lr-3_model_weight.pt
  model_weight_dir: checkpoint/
  checkpoint_path: checkpoint/

model:
  encoder:
    word_embedding_size: 200

global:
  random_seed: 123
  model: SeaReader_v4_5   #SeaReader
   #match-lstm+ # 'match-lstm', 'match-lstm+', 'r-net' or 'base'. Note that 'base' model is customized by base_model.yaml

train:
  continue: False # 默认是不是继续上次训练
  keep_embedding: True # 是否保留weight里的embedding
  batch_size: 100  #最好是5的倍数，便于计算准确率
  valid_batch_size: 100 #default 32
  test_batch_size: 100
  epoch: 50
  enable_cuda: True

  optimizer: 'adam'  # adam, sgd, adamax, adadelta(default is adamax)
  learning_rate: 0.001  # only for sgd
  eps: 1e-6
  clip_grad_norm: 5

test:
  model: SeaReader_v3
  test_batch_size: 100
  enable_cuda: True
  dataset_h5: data/v6fordebug_600_with366k_embedding_qas100_cont100_q60_a12_withlogicAndCategory.h5  #保存预处理的数据，embedding，w2id等
  model_path: checkpoint/panda2_2018-07-28-17_07_30_continue_panda2_2018-07-26-17_35_57,using31wdata100contentlen_model_weight.pt
  output_file_path: 'result/'

